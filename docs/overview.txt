Overview of the MTT Architecture and an Introduction to its Use
Andrew Friedley

--> Warning: while much of the general information in this document is
    correct, some of the specific field names and INI syntax is no
    longer correct.  Be sure to cross-reference against the same INI
    files.

MTT is seriously lacking in user-level documentation, so this document
is a first attempt and remedying that.  MTT takes nearly all of its
input from an INI file supplied by the user, with very few command
line arguments.  The format of the configuration file is as you would
expect for any standard INI file.


Overview
--------

MTT takes nearly all of its input from a configuration file supplied
by the user, with very few command line arguments.  The format of the
configuration file follows a standard INI format, chosen for its ease
of use.

The overall format of the configuration file closely reflects the
internal layout of MTT.  That is, sections of the INI file correspond
to specific actions carried out by the tool.


Stages
------

MTT is split into a number of stages, with each stage carrying out a
unique part of the testing process.  Below is a list of all the
stages, which occur more or less in order:

MPI Get
MPI Install
Test Get
Test Install
Test Run
Reporting

For any particular stage, multiple sections may be written in the
configuration file.  For example, if several different MPI's will be
tested, several MPI Get and MPI Install sections will be specified.
To specify several MPI Get's, a configuration like the following would
be used:

[MPI get: ompi-nightly-trunk]
mpi_name = Open MPI
mpi_installer = Open MPI build

module = OMPI_Snapshot
ompi_snapshot_url = http://www.open-mpi.org/nightly/trunk

[MPI get: ompi svn trunk]
mpi_name = Open MPI
mpi_installer = Open MPI build

module = SVN
svn_url = http://svn.open-mpi.org/svn/ompi/trunk
svn_post_export = <<EOT
./autogen.sh
EOT


The above configuration specifies two actions for the MPI Get stage to
carry out.  The first section specifies that MTT should grab a nightly
Open MPI tarball.  The second section specifies that a check out of
the Open MPI trunk should be obtained.  More detail on the MPI Get
stage is given in the next section.

Note that each section name is prefixed with 'MPI Get: '.  This is how
you specify which MTT stage should carry out the work specified in
this section.  It also affects how the field/value pairs are
interpreted.  Any text in the section name following the stage name
specifies a name for that section, and must be unique.

Also, note the special syntax used for the post_export field in the
second section.  Shell-style line continuation syntax is supported for
multi-line values.  This is especially useful for specifying shell
scripts in the INI file.  In the example, only one command is run, but
any shell scripting syntax may be used here.

One of the most important features of MTT is its modularity.  With
command line arguments, a user may select exactly which stages should
be run (all stages are run by default).  Also, each stage in MTT
allows use of a module to assist in carrying out the work for that
stage.  For example, a module exists for compilation of the Open MPI
modified Intel test suite.  Generic 'simple' modules also exist to
cover the base case, which usually offer support for executing a shell
script.

The MTT module interface was designed with ease of use in mind.  We
expect users to be able to write modules as needed for situations we
have not already covered.  The idea is that MTT should be easily
extended to support any MPI and/or testing application.

In the example above, different modules are used for obtaining a
tarball and obtaining a subversion checkout.


MPI Get
-------

MPI Get, as its name implies, is responsible for getting an MPI.  This
stage handles the logistics involved in getting the source code for an
MPI and bringing it into the MTT scratch area for the current run.
This source is then made available for the MPI Install stage.  Binary
tarballs are also supported, but are not the focus of MTT, nor is the
functionality commonly used or tested.

Different modules are used depending on the source and format of the
MPI.  Tarballs, unpacked source trees, and subversion repositories are
currently supported.  Tarballs and subversion repositories may either
be local or located somewhere on the internet.

An MPI Get example is given above.  The work for each MPI Get section
will be carried out before MTT moves on to the MPI Install stage.

MPI Install
-----------

The MPI Install stage is responsible for compiling a particular MPI
for later use by testing applications.  A many-to-one mapping exists
from MPI Install sections to MPI Get sections.  That is, several MPI
Install sections may refer to a single MPI Get section as the source
for the source code to be used.  This allows for a single MPI code
base to be built in multiple ways.  For example, we commonly build
Open MPI with both GCC and ICC.  An example:

[MPI Install: odin gcc]
mpi_name = Open MPI
mpi_get = ompi svn trunk
perfbase_xml = inp_mpi_install.xml

module = OMPI
vpath_mode = none
compiler_name = gnu
merge_stdout_stderr = 0
compiler_version = &shell("gcc --version | head -n 1 | awk '{ print \$3 }'")
configure_arguments = CFLAGS=-g --enable-debug --disable-mpi-f90

[MPI Install: odin icc]
mpi_name = Open MPI
mpi_get = ompi svn trunk
perfbase_xml = inp_mpi_install.xml

module = OMPI
vpath_mode = none
compiler_name = icc
merge_stdout_stderr = 0
compiler_version = &shell("icc --version | head -n 1 | awk '{ print \$3 }'")
configure_arguments = CFLAGS="-g --enable-debug --disable-mpi-f90" CC=icc


Note the 'MPI Install:' prefix in the section names, indicating these
sections specify work for the MPI Install stage.  Also, the OMPI
module is used here because we are compiling Open MPI.  Other modules
include a Simple module that requires a shell script to compile the
MPI, as well as a CopyTree module to copy an existing binary MPI
installation.

Another important note is that for each MPI Install, the source tree
from the MPI Get stage is copied.  This keeps two MPI Install sections
from stepping on each other, as they use completely separate (but
initially identical) source trees.


Test Get & Test Build
-----------------------

The Test Get and Test Build phases are analogous to the MPI Get and
MPI Install phases, except they refer to test suites/applications.  As
expected, the Test Get stage is responsible for retrieving test
suite/application sources, and the Test Build stage is responsible for
compiling test code.

This is the point where another big feature of MTT comes into play -
what we call the combinatorial effect.  One of the big problems with
running tests by hand is the large number of possible combinations of
MPI installations, configurations, test parameters, and more.  In the
Test Build stage, each Test Build section specified in the
configuration is executed against every successful MPI Install.  If
you have two MPI's and three test suites configured, six test suite
installations will be built during this stage -- each test suite is
built twice, one for each MPI installation.

Otherwise, Test Get & Test Build behave the same as the MPI Get and
MPI Install stages.

Test Run
--------

This stage is where all the magic happens.  A number of parameters can
be specified here, such as a list of processor counts to be used for
each mpirun.  This again adds to the combinatorial affect.  Every test
build is run with every combination of supplied parameters.  Caution
must be used, as this leads to an explosive growth in the number of
tests that are actually run.

[Test Run: intel coll]
test_build = intel
perfbase_xml = inp_test_run.xml
pass = &eq(&test_exit_status(), 0)
timeout = &max(30, &multiply(10, &test_np()))
save_stdout_on_pass = 0
separate_stdout_stderr = 0

module = Simple
tests = src/&cat("coll")
np = &split("2, 4, 8, 16")
np_ok = 0
only_if_exec_exists = 1


