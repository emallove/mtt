#
# Copyright (c) 2006 Cisco Systems, Inc.  All rights reserved.
#

# Template MTT configuration file for Open MPI core testers.  The
# intent for this template file is to establish at least some loose
# guidelines for what Open MPI core testers should be running on a
# nightly basis.  This file is not intended to be an exhaustive sample
# of all possible fields and values that MTT offers.  Each site will
# undoubtedly have to edit this template for their local needs (e.g.,
# pick compilers to use, etc.), but this file provides a baseline set
# of configurations that we intend you to run.

# OMPI core members will need to edit some values in this file based
# on your local testing environment.  Look for comments with "OMPI
# Core:" for instructions on what to change.

# Also keep in mind that at the time of this writing, MTT is still
# under active development and therefore the baselines established in
# this file may change on a relatively frequent basis.

# The guidelines are as follows:
#
# 1. Download and test nightly snapshot tarballs of at least one of
#    the following:
#    - the trunk (highest preference)
#    - release branches (highest preference is the most recent release
#      branch; lowest preference is the oldest release branch)
# 2. Run all 4 correctness test suites from the ompi-tests SVN
#    - trivial, as many processes as possible
#    - intel tests with all_tests_no_perf, up to 64 processes
#    - IBM, as many processes as possible
#    - IMB, as many processes as possible
# 3. Run with as many different components as possible
#    - PMLs (ob1, dr)
#    - BTLs (iterate through sm, tcp, whatever high speed network you
#      have, etc. -- as relevant)

#======================================================================
# Overall configuration
#======================================================================

[MTT]
# No overrides to defaults

#======================================================================
# MPI get phase
#======================================================================

# OMPI Core: If you do not want to test nightly v1.2 tarballs, delete
# this section.  WE STRONGLY ADVISE EVERYONE TO TEST v1.2 TARBALLS!!
[MPI get: ompi-nightly-v1.2]
mpi_details = Open MPI

module = OMPI_Snapshot
ompi_snapshot_url = http://www.open-mpi.org/nightly/v1.2

#----------------------------------------------------------------------

# OMPI Core: If you do not want to test nightly trunk tarballs, delete
# this section.
[MPI get: ompi-nightly-trunk]
mpi_details = Open MPI

module = OMPI_Snapshot
ompi_snapshot_url = http://www.open-mpi.org/nightly/trunk

#----------------------------------------------------------------------

# OMPI Core: If you want to test nightly v1.1 tarballs, uncomment this
# section.
#[MPI get: ompi-nightly-v1.1]
#mpi_details = Open MPI
#module = OMPI_Snapshot
#ompi_snapshot_url = http://www.open-mpi.org/nightly/v1.1

#----------------------------------------------------------------------

# OMPI Core: If you want to test nightly v1.0 tarballs, uncomment this
# section.
#[MPI get: ompi-nightly-v1.0]
#mpi_details = Open MPI
#module = OMPI_Snapshot
#ompi_snapshot_url = http://www.open-mpi.org/nightly/v1.0

#======================================================================
# Install MPI phase
#======================================================================

[MPI install: odin gcc warnings]
# OMPI Core: If you deleted any of the "MPI Get" sections (above),
# ensure to delete their corresponding entries in the "mpi_get" line
# below:
mpi_get = ompi-nightly-trunk,ompi-nightly-v1.2,ompi-nightly-v1.1,ompi-nightly-v1.0
save_stdout_on_success = 1
separate_stdout_stderr = 1
vpath_mode = none
# OMPI Core: This is a GNU make option; if you are not using GNU make,
# you'll probably want to delete this field (i.e., leave it to its
# default [empty] value).
make_all_arguments = -j 4
make_check = 1
# OMPI Core: You will likely need to update these values for whatever
# compiler you want to use.  You can pass any configure flags that you
# want, including those that change which compiler to use (e.g., CC=cc
# CXX=CC F77=f77 FC=f90).  Valid compiler names are: gnu, pgi, intel,
# ibm, kai, absoft, pathscale, sun.  If you have other names that you
# need, let us know.  Note that the compiler_name flag is solely for
# classifying test results; it does not automatically pass values to
# configure to set the compiler.
compiler_name = gnu
compiler_version = &shell("gcc --version | head -n 1 | awk '{ print \$3 }'")
configure_arguments = CFLAGS=-g --enable-picky --enable-debug

module = OMPI

#======================================================================
# MPI run details
#======================================================================

[MPI Details: Open MPI]
exec = mpirun -np &test_np() --prefix &test_prefix() &test_executable() &test_argv()

after_each_exec = <<EOT

# This scriptlet ensures that all remnants of the prior mpirun are
# gone.  It kills all orteds running under this user and whacks any
# session directories that it finds.  Hence, do not expect to be able
# to run on the same machine/user as a user who is running MTT tests.

# This scriptlet is not fully tested yet.  Needs testing on: Linux,
# OSX, Solaris.

who=`whoami`
which killall > /dev/null 2> /dev/null
if test "$?" = "0"; then
    # If we have killall, it's easy.
    killall -9 orted
else
    # We're on an OS without killall.  Which variant of ps do we have?
    ps auxw > /dev/null 2> /dev/null
    if test "$?" = "0"; then
        ps_args="auxww"
    else
        ps_args="-eadf"
    fi
    pids=`ps $ps_args | grep $who | grep -v grep | grep orted awk '{ print $2 }'`
    if test "$pids" != ""; then
        kill -9 $pids
    fi
fi

# Whack any remaining session directories.  This is a workaround for
# current bugs in OMPI.
rm -rf /tmp/openmpi-sessions-${who}*

EOT

#======================================================================
# Test get phase
#======================================================================

[Test get: trivial]
module = Trivial

#----------------------------------------------------------------------

[Test get: intel]
module = SVN
svn_url = https://svn.open-mpi.org/svn/ompi-tests/trunk/intel_tests

#----------------------------------------------------------------------

[Test get: ibm]
module = SVN
svn_url = https://svn.open-mpi.org/svn/ompi-tests/trunk/ibm
svn_post_export = <<EOT
./autogen.sh
EOT

#----------------------------------------------------------------------

[Test get: imb]
module = SVN
svn_url = https://svn.open-mpi.org/svn/ompi-tests/trunk/IMB_2.3

#======================================================================
# Test build phase
#======================================================================

[Test build: trivial]
test_get = trivial
save_stdout_on_success = 1
separate_stdout_stderr = 1
stderr_save_lines = -1

module = Trivial

#----------------------------------------------------------------------

[Test build: intel]
test_get = intel
save_stdout_on_success = 1
separate_stdout_stderr = 1
stderr_save_lines = -1

module = Intel_OMPI_Tests
intel_ompi_tests_buildfile = all_tests_no_perf
# I am using the g77 compiler, which needs these flags.  Your fortran
# compiler may need different flags.
intel_ompi_tests_fflags = -g -fugly-complex -fno-globals -Wno-globals -Isrc -I.

#----------------------------------------------------------------------

[Test build: ibm]
test_get = ibm
save_stdout_on_success = 1
separate_stdout_stderr = 1
stderr_save_lines = -1

module = Shell
shell_build_command = <<EOT
./configure CC=mpicc CXX=mpic++ F77=mpif77
make
EOT

#----------------------------------------------------------------------

[Test build: imb]
test_get = imb
save_stdout_on_success = 1
separate_stdout_stderr = 1
stderr_save_lines = -1

module = Shell
shell_build_command = <<EOT
cd src
make clean IMB-MPI1
EOT

#======================================================================
# Test Run phase
#======================================================================

[Test run: trivial]
test_build = trivial
pass = &eq(&test_exit_status(), 0)
timeout = &max(30, &multiply(10, &test_np()))
save_stdout_on_pass = 1
separate_stdout_stderr = 1
np = &if(&gt(&rm_max_procs(), 0), &step(2, &max(2, &rm_max_procs()), 2), 2)

module = Simple
simple_tests = &find_executables(".")
simple_only_if_exec_exists = 1

#----------------------------------------------------------------------

[Test run: intel]
test_build = intel
pass = &eq(&test_exit_status(), 0)
timeout = &max(30, &multiply(10, &test_np()))
save_stdout_on_pass = 1
separate_stdout_stderr = 1
# The intel tests have some hard limits at 64 processes
np = &min(64, &rm_max_procs()),

module = Simple
simple_tests = &find_executables("src")
simple_only_if_exec_exists = 1

#----------------------------------------------------------------------

[Test run: ibm]
test_build = ibm
pass = &or(&eq(&test_exit_status(), 0), &eq(&test_exit_status(), 77))
timeout = &max(30, &multiply(10, &test_np()))
save_stdout_on_pass = 1
separate_stdout_stderr = 1

module = Simple
simple_tests = &find_executables("collective", "communicator", "datatype", \
                          "dynamic", "environment", "group", "info", \
                          "io", "onesided", "pt2pt", "topology")

#----------------------------------------------------------------------

[Test run: imb]
test_build = imb
pass = &eq(&test_exit_status(), 0)
timeout = &max(1800, &multiply(50, &test_np()))
save_stdout_on_pass = 1
separate_stdout_stderr = 1

module = Simple
simple_tests = src/IMB-MPI1

#======================================================================
# Reporter phase
#======================================================================

[Reporter: IU database]
module = Perfbase

perfbase_realm = OMPI
perfbase_url = http://www.open-mpi.org/mtt/submit/
# OMPI Core: Change this to be the username and password for your
# submit user.  Get this from the OMPI MTT administrator.
perfbase_username = you must set this value
perfbase_password = you must set this value
# OMPI Core: Change this to be some short string identifying your
# cluster.
perfbase_platform = you must set this value

#----------------------------------------------------------------------

# This is a backup for while debugging MTT; it also writes results to
# a local text file
[Reporter: text file backup]
module = TextFile

textfile_filename = odin-$phase-$section-$mpi_name-$mpi_version.txt
textfile_separator = >>>>----------------------------------------------------------<<<<

