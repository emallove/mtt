#
# Copyright (c) 2006-2007 Cisco Systems, Inc.  All rights reserved.
# Copyright (c) 2006-2007 Sun Microystems, Inc.  All rights reserved.
#

# Template MTT configuration file for LAM/MPI core testers.  The
# intent for this template file is to establish at least some loose
# guidelines for what Open MPI core testers should be running on a
# nightly basis.  This file is not intended to be an exhaustive sample
# of all possible fields and values that MTT offers.  Each site will
# undoubtedly have to edit this template for their local needs (e.g.,
# pick compilers to use, etc.), but this file provides a baseline set
# of configurations that we intend you to run.

# LAM/MPI core members will need to edit some values in this file based
# on your local testing environment.  Look for comments with "LAM/MPI
# Core:" for instructions on what to change.

# Note that this file is artificially longer than it really needs to
# be -- a bunch of values are explicitly set here that are exactly
# equivalent to their defaults.  This is mainly because there is no
# reliable form of documentation for this ini file yet, so the values
# here comprise a good set of what options are settable (although it
# is not a comprehensive set).

# Also keep in mind that at the time of this writing, MTT is still
# under active development and therefore the baselines established in
# this file may change on a relatively frequent basis.

# The guidelines are as follows:
#
# 1. Download and test nightly snapshot tarballs of at least one of
#    the following:
#    - the trunk (highest preference)
#    - release branches (highest preference is the most recent release
#      branch; lowest preference is the oldest release branch)
# 2. Run all 4 correctness test suites from the ompi-tests SVN
#    - trivial, as many processes as possible
#    - intel tests with all_tests_no_perf, up to 64 processes
#    - IBM, as many processes as possible
#    - IMB, as many processes as possible
# 3. Run with as many different components as possible
#    - PMLs (ob1, dr)
#    - BTLs (iterate through sm, tcp, whatever high speed network(s) you
#      have, etc. -- as relevant)

#======================================================================
# Overall configuration
#======================================================================

[MTT]

# LAM/MPI Core: if you are not running in a scheduled environment and you
# have a fixed hostfile for what nodes you'll be running on, fill in
# the absolute pathname to it here.  If you do not have a hostfile,
# leave it empty.  Example:
#     hostfile = /home/me/mtt-runs/mtt-hostfile
# This file will be parsed and will automatically set a valid value
# for &env_max_np() (it'll count the number of lines in the hostfile,
# adding slots/cpu counts if it finds them).  The "hostfile" value is
# ignored if you are running in a recognized scheduled environment.
hostfile =

# LAM/MPI Core: if you would rather list the hosts individually on the
# mpirun command line, list hosts here delimited by whitespace (if you
# have a hostfile listed above, this value will be ignored!).  Hosts
# can optionally be suffixed with ":num", where "num" is an integer
# indicating how many processes may be started on that machine (if not
# specified, ":1" is assumed).  The sum of all of these values is used
# for &env_max_np() at run time.  Example (4 uniprocessors):
#    hostlist = node1 node2 node3 node4
# Another example (4 2-way SMPs):
#    hostlist = node1:2 node2:2 node3:2 node4:2
# The "hostlist" value is ignored if you are running in a scheduled
# environment or if you have specified a hostfile.
hostlist =

# LAM/MPI Core: if you are running in a scheduled environment and want to
# override the scheduler and set the maximum number of processes
# returned by &env_max_procs(), you can fill in an integer here.
max_np = 

# LAM/MPI Core: Output display preference; the default width at which MTT
# output will wrap.
textwrap = 76

# LAM/MPI Core: After the timeout for a command has passed, wait this
# many additional seconds to drain all output, and then kill it with
# extreme prejiduce.
drain_timeout = 5

# LAM/MPI Core: Whether this invocation of the client is a test of the
# client setup itself.  Specifically, this value should be set to true
# (1) if you are testing your MTT client and/or INI file and do not
# want the results included in normal reporting in the MTT central
# results database.  Results submitted in "trial" mode are not
# viewable (by default) on the central database, and are automatically
# deleted from the database after a short time period (e.g., a week).
# Setting this value to 1 is exactly equivalent to passing "--trial"
# on the MTT client command line.  However, any value specified here
# in this INI file will override the "--trial" setting on the command
# line (i.e., if you set "trial = 0" here in the INI file, that will
# override and cancel the effect of "--trial" on the command line).
# trial = 0

# LAM/MPI Core: Set the scratch parameter here (if you do not want it to
# be automatically set to your current working directory). Setting
# this parameter accomplishes the same thing that the --scratch option
# does.
# scratch = &getenv("HOME")/mtt-scratch

#======================================================================
# MPI get phase
#======================================================================

[MPI get: lam-nightly-trunk]
mpi_details = LAM

module = LAM_Snapshot
lam_snapshot_url = http://www.lam-mpi.org/download/files/nightly

#======================================================================
# Install MPI phase
#======================================================================

[MPI install: LAM/GNU]
mpi_get = lam-nightly-trunk
save_stdout_on_success = 1
merge_stdout_stderr = 0

module = LAM
lam_vpath_mode = none
lam_make_all_arguments = -j 8
lam_compiler_name = gnu
lam_compiler_version = &get_gcc_version()
lam_configure_arguments = "CFLAGS=-g -pipe" --with-boot-tm=/opt/pbs

#----------------------------------------------------------------------

[MPI install: LAM/Intel 9.0]
mpi_get = lam-nightly-trunk
save_stdout_on_success = 1
merge_stdout_stderr = 0
env_module = intel-compilers/9.0

module = LAM
lam_vpath_mode = none
lam_make_all_arguments = -j 8
lam_compiler_name = intel
lam_compiler_version = &get_icc_version()
lam_configure_arguments = CC=icc CXX=icpc F77=ifort FC=ifort "CFLAGS=-g -wd188" --enable-picky --enable-debug --with-openib=/usr/local/ofed --with-udapl=/usr/local/ofed --with-tm=/opt/pbs

#----------------------------------------------------------------------

[MPI install: LAM/Intel 9.1]
mpi_get = lam-nightly-trunk
save_stdout_on_success = 1
merge_stdout_stderr = 0
env_module = intel-compilers/9.1

module = LAM
lam_vpath_mode = none
lma_make_all_arguments = -j 8
lam_compiler_name = intel
lam_compiler_version = &get_icc_version()
lam_configure_arguments = CC=icc CXX=icpc F77=ifort FC=ifort "CFLAGS=-g -wd188" --enable-picky --enable-debug --with-openib=/usr/local/ofed --with-udapl=/usr/local/ofed --with-tm=/opt/pbs

#----------------------------------------------------------------------

[MPI install: LAM/PGI 6.2]
mpi_get = lam-nightly-trunk
save_stdout_on_success = 1
merge_stdout_stderr = 0
env_module = pgi-compilers/6.2.5

module = LAM
lam_vpath_mode = none
lam_make_all_arguments = -j 1
lam_compiler_name = pgi
lam_compiler_version = &get_pgcc_version()
lam_configure_arguments = CC=pgcc CXX=pgCC F77=pgf77 FC=pgf90 CFLAGS=-g --enable-picky --enable-debug --with-openib=/usr/local/ofed --with-udapl=/usr/local/ofed --with-tm=/opt/pbs --with-wrapper-cxxflags=-fPIC

#----------------------------------------------------------------------

[MPI install: LAM/PGI 7.0.2]
mpi_get = lam-nightly-trunk
save_stdout_on_success = 1
merge_stdout_stderr = 0
env_module = pgi-compilers/7.0.2

module = LAM
lam_vpath_mode = none
lam_make_all_arguments = -j 1
lam_compiler_name = pgi
lam_compiler_version = &get_pgcc_version()
lam_configure_arguments = CC=pgcc CXX=pgCC F77=pgf77 FC=pgf90 CFLAGS=-g --enable-picky --enable-debug --with-openib=/usr/local/ofed --with-udapl=/usr/local/ofed --with-tm=/opt/pbs --with-wrapper-cxxflags=-fPIC

#----------------------------------------------------------------------

[MPI install: LAM/Pathscale 3.0]
mpi_get = lam-nightly-trunk
save_stdout_on_success = 1
merge_stdout_stderr = 0
env_module = pathscale-compilers/3.0

module = LAM
lam_vpath_mode = none
lam_make_all_arguments = -j 4
lam_compiler_name = pathscale
lam_compiler_version = &get_pathcc_version()
lam_configure_arguments = CC=pathcc CXX=pathCC FC=pathf90 F77=pathf90 --enable-picky --enable-debug --with-openib=/usr/local/ofed --with-udapl=/usr/local/ofed --with-tm=/opt/pbs 

#======================================================================
# MPI run details
#======================================================================

[MPI Details: Open MPI]
# LAM is nice and simple!
before_any_exec = lamboot -s

exec = mpirun C &test_executable() &test_argv()

after_each_exec = lamclean

after_all_exec = lamhalt

#======================================================================
# Test get phase
#======================================================================

[Test get: trivial]
module = Trivial

#----------------------------------------------------------------------

[Test get: ibm]
module = SVN
svn_url = https://svn.open-mpi.org/svn/ompi-tests/trunk/ibm
svn_post_export = <<EOT
./autogen.sh
EOT

#----------------------------------------------------------------------

[Test get: mpicxx]
module = SVN
svn_url = https://svn.open-mpi.org/svn/ompi-tests/trunk/cxx-test-suite
svn_post_export = <<EOT
./autogen.sh
EOT

#----------------------------------------------------------------------

[Test get: netpipe]
module = SVN
svn_url = https://svn.open-mpi.org/svn/ompi-tests/trunk/NetPIPE_3.6.2

#======================================================================
# Test build phase
#======================================================================

[Test build: trivial]
test_get = trivial
save_stdout_on_success = 1
merge_stdout_stderr = 1
stderr_save_lines = 100

module = Trivial

#----------------------------------------------------------------------

[Test build: ibm]
test_get = ibm
save_stdout_on_success = 1
merge_stdout_stderr = 1
stderr_save_lines = 100

module = Shell
shell_build_command = <<EOT
./configure CC=mpicc CXX=mpic++ F77=mpif77
make
EOT

#----------------------------------------------------------------------

[Test build: mpicxx]
test_get = mpicxx
save_stdout_on_success = 1
merge_stdout_stderr = 1

module = Shell
shell_build_command = <<EOT
./configure CC=mpicc CXX=mpic++
make
EOT

#----------------------------------------------------------------------

[Test build: netpipe]
test_get = netpipe
save_stdout_on_success = 1
merge_stdout_stderr = 1
stderr_save_lines = 100

module = Shell
shell_build_command = <<EOT
make mpi
EOT

#======================================================================
# Test Run phase
#======================================================================

[Test run: trivial]
test_build = trivial
pass = &and(&test_wifexited(), &eq(&test_wexitstatus(), 0))
timeout = &test_np()
save_stdout_on_pass = 1
merge_stdout_stderr = 1
stdout_save_lines = 100
np = &env_max_procs()

specify_module = Simple
simple_only:tests = &find_executables(".")

#----------------------------------------------------------------------

[Test run: ibm]
test_build = ibm
pass = &and(&test_wifexited(), &eq(&test_wexitstatus(), 0))
skipped = &and(&test_wifexited(), &eq(&test_wexitstatus(), 77))
timeout = &max(30, &multiply(10, &test_np()))
save_stdout_on_pass = 1
merge_stdout_stderr = 1
stdout_save_lines = 100
np = &env_max_procs()

specify_module = Simple
# Similar rationale to the intel test run section
simple_first:tests = &find_executables("collective", "communicator", \
                                       "datatype", "dynamic", "environment", \
                                       "group", "info", "io", "onesided", \
                                       "pt2pt", "topology")

# Similar rationale to the intel test run section
simple_fail:tests = environment/abort environment/final
simple_fail:pass = &and(&test_wifexited(), &ne(&test_wexitstatus(), 0))
simple_fail:exclusive = 1
simple_fail:np = &env_max_procs()

#----------------------------------------------------------------------

[Test run: mpicxx]
test_build = mpicxx
pass = &and(&test_wifexited(), &eq(&test_wexitstatus(), 0))
timeout = &max(30, &multiply(10, &test_np()))
save_stdout_on_pass = 1
merge_stdout_stderr = 1
argv = &if(&eq("&mpi_get_name()", "ompi-nightly-v1.1"), "-nothrow", "")
np = &env_max_procs()

specify_module = Simple
simple_pass:tests = src/mpi2c++_test

#----------------------------------------------------------------------

[Test run: netpipe]
test_build = netpipe
pass = &eq(&test_wexitstatus(), 0)
timeout = -1
save_stdout_on_pass = 1
# Ensure to leave this value as "-1", or performance results could be lost!
stdout_save_lines = -1
merge_stdout_stderr = 1
# NetPIPE is ping-pong only, so we only need 2 procs
np = 2

specify_module = Simple
analyze_module = NetPipe
simple_pass:tests = NPmpi

#======================================================================
# Reporter phase
#======================================================================

[Reporter: IU database]
module = MTTDatabase

mttdatabase_realm = OMPI
mttdatabase_url = https://www.open-mpi.org/mtt/submit/
# LAM/MPI Core: Change this to be the username and password for your
# submit user.  Get this from the LAM/MPI MTT administrator.
mttdatabase_username = you must set this value
mttdatabase_password = you must set this value
# LAM/MPI Core: Change this to be some short string identifying your
# cluster.
mttdatabase_platform = you must set this value

#----------------------------------------------------------------------

# This is a backup for while debugging MTT; it also writes results to
# a local text file

[Reporter: text file backup]
module = TextFile

textfile_filename = $phase-$section-$mpi_name-$mpi_version.txt

textfile_summary_header = <<EOT
hostname: &shell("hostname")
uname: &shell("uname -a")
who am i: &shell("who am i")
EOT

textfile_summary_footer =
textfile_detail_header =
textfile_detail_footer =

textfile_textwrap = 78
