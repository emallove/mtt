#======================================================================
# Cisco configuration
#======================================================================

[MTT]

#----------------------------------------------------------------------

[Lock]

#======================================================================
# MPI get phase
#======================================================================

[MPI get: ompi-nightly-trunk]
mpi_details = OMPI

module = OMPI_Snapshot
ompi_snapshot_url = http://www.open-mpi.org/nightly/trunk

#----------------------------------------------------------------------

[MPI get: ompi-nightly-v1.2]
mpi_details = OMPI

module = OMPI_Snapshot
ompi_snapshot_url = http://www.open-mpi.org/nightly/v1.2

#======================================================================
# Install MPI phase
#======================================================================

[MPI install: OMPI/GNU-standard]
mpi_get = ompi-nightly-v1.2,ompi-nightly-trunk
save_stdout_on_success = 1
merge_stdout_stderr = 0

module = OMPI
ompi_vpath_mode = none
ompi_make_all_arguments = -j 8
ompi_make_check = 1
ompi_compiler_name = gnu
ompi_compiler_version = &get_gcc_version()
ompi_configure_arguments = "CFLAGS=-g -pipe" --enable-picky --enable-debug --with-openib=/usr/local/ofed --with-udapl=/usr/local/ofed --with-tm=/opt/pbs --enable-mpirun-prefix-by-default

#----------------------------------------------------------------------

[MPI install: OMPI/Intel-9.0]
mpi_get = ompi-nightly-v1.2,ompi-nightly-trunk
save_stdout_on_success = 1
merge_stdout_stderr = 0
env_module = intel-compilers/9.0

module = OMPI
ompi_vpath_mode = none
ompi_make_all_arguments = -j 8
ompi_make_check = 1
ompi_compiler_name = intel
ompi_compiler_version = &get_icc_version()
ompi_configure_arguments = CC=icc CXX=icpc F77=ifort FC=ifort "CFLAGS=-g -wd188" --enable-picky --enable-debug --with-openib=/usr/local/ofed --with-udapl=/usr/local/ofed --with-tm=/opt/pbs --enable-mpirun-prefix-by-default

#----------------------------------------------------------------------

[MPI install: OMPI/Intel-9.1]
mpi_get = ompi-nightly-v1.2,ompi-nightly-trunk
save_stdout_on_success = 1
merge_stdout_stderr = 0
env_module = intel-compilers/9.1

module = OMPI
ompi_vpath_mode = none
ompi_make_all_arguments = -j 8
ompi_make_check = 1
ompi_compiler_name = intel
ompi_compiler_version = &get_icc_version()
ompi_configure_arguments = CC=icc CXX=icpc F77=ifort FC=ifort "CFLAGS=-g -wd188" --enable-picky --enable-debug --with-openib=/usr/local/ofed --with-udapl=/usr/local/ofed --with-tm=/opt/pbs --enable-mpirun-prefix-by-default

#----------------------------------------------------------------------

[MPI install: OMPI/PGI-6.2]
mpi_get = ompi-nightly-v1.2,ompi-nightly-trunk
save_stdout_on_success = 1
merge_stdout_stderr = 0
env_module = pgi-compilers/6.2.5

module = OMPI
ompi_vpath_mode = none
ompi_make_all_arguments = -j 1
ompi_make_check = 1
ompi_compiler_name = pgi
ompi_compiler_version = &get_pgcc_version()
ompi_configure_arguments = CC=pgcc CXX=pgCC F77=pgf77 FC=pgf90 CFLAGS=-g --enable-picky --enable-debug --with-openib=/usr/local/ofed --with-udapl=/usr/local/ofed --with-tm=/opt/pbs --enable-mpirun-prefix-by-default --with-wrapper-cxxflags=-fPIC

#----------------------------------------------------------------------

[MPI install: OMPI/PGI-7.0.2]
mpi_get = ompi-nightly-v1.2,ompi-nightly-trunk
save_stdout_on_success = 1
merge_stdout_stderr = 0
env_module = pgi-compilers/7.0.2

module = OMPI
ompi_vpath_mode = none
ompi_make_all_arguments = -j 1
ompi_make_check = 1
ompi_compiler_name = pgi
ompi_compiler_version = &get_pgcc_version()
ompi_configure_arguments = CC=pgcc CXX=pgCC F77=pgf77 FC=pgf90 CFLAGS=-g --enable-picky --enable-debug --with-openib=/usr/local/ofed --with-udapl=/usr/local/ofed --with-tm=/opt/pbs --enable-mpirun-prefix-by-default --with-wrapper-cxxflags=-fPIC

#----------------------------------------------------------------------

[MPI install: OMPI/Pathscale-3.0]
mpi_get = ompi-nightly-v1.2,ompi-nightly-trunk
save_stdout_on_success = 1
merge_stdout_stderr = 0
env_module = pathscale-compilers/3.0

module = OMPI
ompi_vpath_mode = none
ompi_make_all_arguments = -j 4
ompi_make_check = 1
ompi_compiler_name = pathscale
ompi_compiler_version = &get_pathcc_version()
ompi_configure_arguments = CC=pathcc CXX=pathCC FC=pathf90 F77=pathf90 --enable-picky --enable-debug --with-openib=/usr/local/ofed --with-udapl=/usr/local/ofed --with-tm=/opt/pbs --enable-mpirun-prefix-by-default --disable-mpi-cxx

#======================================================================
# MPI run details
#======================================================================

[MPI Details: OMPI]
exec = mpirun -np &test_np() @mca@ --mca btl_tcp_if_include ib0 --mca oob_tcp_if_include ib0 &test_executable() &test_argv()

# JMS fix this up
mca = &enumerate( \
        "--mca btl sm,tcp,self @common_params@", \
        "--mca btl tcp,self @common_params@", \
        "--mca btl sm,openib,self @common_params@", \
        "--mca btl openib,self @common_params@", \
        "--mca mpi_leave_pinned 1 --mca btl openib,self @common_params@", \
        "--mca mpi_leave_pinned_pipeline 1 --mca btl openib,self @common_params@", \
        "--mca btl_openib_use_eager_rdma 0 --mca btl openib,self @common_params@", \
        "--mca btl_openib_use_srq 1 --mca btl openib,self @common_params@", \
        "--mca mpi_leave_pinned 1 --mca btl sm,openib,self @common_params@" )

common_params = 

# It is important that the after_each_exec step is a single
# command/line so that MTT will launch it directly (instead of via a
# temporary script).  This is because the "srun" command is
# (intentionally) difficult to kill in some cases.  See
# https://svn.open-mpi.org/trac/mtt/changeset/657 for details.

after_each_exec = &if(&ne("", &getenv("SLURM_NNODES")), "srun -N &getenv("SLURM_NNODES")") /home/mpiteam/cron/after_each_exec.pl

#======================================================================
# Test get phase
#======================================================================

[Test get: imb]
module = SVN
svn_url = https://svn.open-mpi.org/svn/ompi-tests/trunk/IMB_2.3

#----------------------------------------------------------------------

[Test get: netpipe]
module = SVN
svn_url = https://svn.open-mpi.org/svn/ompi-tests/trunk/NetPIPE_3.6.2

#----------------------------------------------------------------------

[Test get: skampi]
module = SVN
svn_url = https://svn.open-mpi.org/svn/ompi-tests/trunk/skampi-5.0.1

#======================================================================
# Test build phase
#======================================================================

[Test build: imb]
test_get = imb
save_stdout_on_success = 1
merge_stdout_stderr = 1

module = Shell
shell_build_command = <<EOT
cd src
make clean IMB-MPI1
EOT

#----------------------------------------------------------------------

[Test build: netpipe]
test_get = netpipe
save_stdout_on_success = 1
merge_stdout_stderr = 1
stderr_save_lines = 100

module = Shell
shell_build_command = <<EOT
make mpi
EOT

#----------------------------------------------------------------------

[Test build: skampi]
test_get = skampi
save_stdout_on_success = 1
merge_stdout_stderr = 1
stderr_save_lines = 100

module = Shell
shell_build_command = <<EOT
make
EOT

#======================================================================
# Test Run phase
#======================================================================

[Test run: imb-performance]
test_build = imb
pass = &eq(&test_wexitstatus(), 0)
timeout = -1
save_stdout_on_pass = 1
# Ensure to leave this value as "-1", or performance results could be lost!
stdout_save_lines = -1
merge_stdout_stderr = 1
np = &env_max_procs()

argv = -npmin &test_np() &enumerate("PingPong", "PingPing", "Sendrecv", "Exchange", "Allreduce", "Reduce", "Reduce_scatter", "Allgather", "Allgatherv", "Alltoall", "Bcast", "Barrier") 

specify_module = Simple
analyze_module = IMB
simple_pass:tests = IMB-MPI1

#----------------------------------------------------------------------

[Test run: netpipe]
test_build = netpipe
pass = &eq(&test_wexitstatus(), 0)
timeout = -1
save_stdout_on_pass = 1
# Ensure to leave this value as "-1", or performance results could be lost!
stdout_save_lines = -1
merge_stdout_stderr = 1
np = 2

specify_module = Simple
analyze_module = NetPipe
simple_pass:tests = NPmpi

#----------------------------------------------------------------------

[Test run: skampi-performance]
test_build = skampi
pass = &eq(&test_wexitstatus(), 0)
timeout = -1
save_stdout_on_pass = 1
# Ensure to leave this value as "-1", or performance results could be lost!
stdout_save_lines = -1
merge_stdout_stderr = 1
np = &env_max_procs()

# Need a funclet for it
argv = -i &find("mtt/", "*.ski")

specify_module = Simple
analyze_module = SKaMPI
simple_pass:tests = skampi

#======================================================================
# Reporter phase
#======================================================================

[Reporter: IU database]
module = MTTDatabase

mttdatabase_realm = OMPI
mttdatabase_url = https://www.open-mpi.org/mtt/submit/
# OMPI Core: Change this to be the username and password for your
# submit user.  Get this from the OMPI MTT administrator.
mttdatabase_username = you must set this value
mttdatabase_password = you must set this value
# OMPI Core: Change this to be some short string identifying your
# cluster.
mttdatabase_platform = you must set this value

#----------------------------------------------------------------------

# This is a backup for while debugging MTT; it also writes results to
# a local text file

[Reporter: text file backup]
module = TextFile

textfile_filename = $phase-$section-$mpi_name-$mpi_version.txt

textfile_summary_header = <<EOT
hostname: &shell("hostname")
uname: &shell("uname -a")
who am i: &shell("who am i")
EOT

textfile_summary_footer =
textfile_detail_header =
textfile_detail_footer =

textfile_textwrap = 78
