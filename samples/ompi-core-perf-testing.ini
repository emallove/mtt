#======================================================================
# Generic OMPI core performance testing template configuration
#======================================================================

[MTT]
# Leave this string so that we can identify this data subset in the
# database
description = [2007 collective performance bakeoff]
# OMPI Core: put values here as relevant to your environment.

#----------------------------------------------------------------------

[Lock]
# OMPI Core: put values here as relevant to your environment.

#======================================================================
# MPI get phase
#======================================================================

[MPI get: ompi-nightly-trunk]
mpi_details = OMPI

module = OMPI_Snapshot
ompi_snapshot_url = http://www.open-mpi.org/nightly/trunk

#----------------------------------------------------------------------

[MPI get: ompi-nightly-v1.2]
mpi_details = OMPI

module = OMPI_Snapshot
ompi_snapshot_url = http://www.open-mpi.org/nightly/v1.2

#----------------------------------------------------------------------

[MPI get: ompi-released-v1.2]
mpi_details = OMPI

module = OMPI_Snapshot
ompi_snapshot_url = http://www.open-mpi.org/software/ompi/v1.2/downloads

#----------------------------------------------------------------------

[MPI get: MPICH1]
mpi_details = MPICH1

module = Download
download_url = http://www-unix.mcs.anl.gov/mpi/mpich1/downloads/mpich.tar.gz
# This version is fixed/frozen, so it's ok to hard-code it
download_version = 1.2.7p1

#----------------------------------------------------------------------

[MPI get: MPICH-MX]
mpi_details = MPICH1

module = Download
download_url = http://www.myri.com/ftp/pub/MPICH-MX/mpich-mx_1.2.7..5.tar.gz
# OMPI Core: you need to obtain the username and password from Myricom
download_username = <OBTAIN THIS FROM MYRICOM>
download_password = <OBTAIN THIS FROM MYRICOM>

#----------------------------------------------------------------------

[MPI get: MPICH2]
mpi_details = MPICH2

module = Download
download_url = http://www-unix.mcs.anl.gov/mpi/mpich2/downloads/mpich2-1.0.5p4.tar.gz

#----------------------------------------------------------------------

[MPI get: MVAPICH]
mpi_details = MPICH1

module = Download
download_url = http://mvapich.cse.ohio-state.edu/download/mvapich/mvapich-0.9.9.tar.gz

#----------------------------------------------------------------------

[MPI get: MVAPICH2]
mpi_details = MPICH2

module = Download
download_url = http://mvapich.cse.ohio-state.edu/download/mvapich2/mvapich2-0.9.8p3.tar.gz

#----------------------------------------------------------------------

[MPI get: HP-MPI]
mpi_details = HP-MPI

# OMPI core: you need to have HP MPI already installed somewhere
module = AlreadyInstalled

#----------------------------------------------------------------------

[MPI get: Intel-MPI]
mpi_details = Intel-MPI

# OMPI core: you need to have HP MPI already installed somewhere
module = AlreadyInstalled

#----------------------------------------------------------------------

[SKIP MPI get: Scali-MPI]
mpi_details = Scali-MPI

# OMPI core: you need to have HP MPI already installed somewhere
module = Scali-MPI

#----------------------------------------------------------------------

[SKIP MPI get: Cray-MPI]
mpi_details = Cray-MPI

# JMS: Need to figure out what to do here (already installed?)
module = Cray-MPI

#======================================================================
# Install MPI phase
#======================================================================

[MPI install: OMPI/GNU-standard]
mpi_get = ompi-nightly-trunk ompi-nightly-v1.2 ompi-released-v1.2
save_stdout_on_success = 1
merge_stdout_stderr = 0

module = OMPI
ompi_vpath_mode = none
ompi_make_all_arguments = -j 8
ompi_make_check = 1
ompi_compiler_name = gnu
ompi_compiler_version = &get_gcc_version()
ompi_configure_arguments = "CFLAGS=-g -pipe" --enable-picky --enable-debug --with-openib=/usr/local/ofed --with-udapl=/usr/local/ofed --with-tm=/opt/pbs --enable-mpirun-prefix-by-default

#----------------------------------------------------------------------

[MPI install: MPICH1]
mpi_get = MPICH
save_stdout_on_success = 1
merge_stdout_stderr = 0

module = MPICH1
# JMS continue here
foo = ......?

#======================================================================
# MPI run details
#======================================================================

[MPI Details: OMPI]
exec = mpirun -np &test_np() @mca@ &test_executable() &test_argv()

# JMS fix this up
mca = &enumerate( \
        "--mca btl sm,tcp,self @common_params@", \
        "--mca btl tcp,self @common_params@", \
        "--mca btl sm,openib,self @common_params@", \
        "--mca btl openib,self @common_params@", \
        "--mca mpi_leave_pinned 1 --mca btl openib,self @common_params@", \
        "--mca mpi_leave_pinned_pipeline 1 --mca btl openib,self @common_params@", \
        "--mca btl_openib_use_eager_rdma 0 --mca btl openib,self @common_params@", \
        "--mca btl_openib_use_srq 1 --mca btl openib,self @common_params@", \
        "--mca mpi_leave_pinned 1 --mca btl sm,openib,self @common_params@" )

common_params = --mca btl_tcp_if_include ib0 --mca oob_tcp_if_include ib0 

# It is important that the after_each_exec step is a single
# command/line so that MTT will launch it directly (instead of via a
# temporary script).  This is because the "srun" command is
# (intentionally) difficult to kill in some cases.  See
# https://svn.open-mpi.org/trac/mtt/changeset/657 for details.

after_each_exec = &if(&ne("", &getenv("SLURM_NNODES")), "srun -N &getenv("SLURM_NNODES")") /home/mpiteam/cron/after_each_exec.pl

#======================================================================
# Test get phase
#======================================================================

[Test get: netpipe]
module = SVN
svn_url = https://svn.open-mpi.org/svn/ompi-tests/trunk/NetPIPE_3.6.2

#----------------------------------------------------------------------

[Test get: imb]
module = SVN
svn_url = https://svn.open-mpi.org/svn/ompi-tests/trunk/IMB_2.3

#----------------------------------------------------------------------

[Test get: osu]
module = SVN
svn_url = https://svn.open-mpi.org/svn/ompi-tests/trunk/osu

#----------------------------------------------------------------------

[Test get: skampi]
module = SVN
svn_url = https://svn.open-mpi.org/svn/ompi-tests/trunk/skampi-5.0.1

#======================================================================
# Test build phase
#======================================================================

[Test build: netpipe]
test_get = netpipe
save_stdout_on_success = 1
merge_stdout_stderr = 1
stderr_save_lines = 100

module = Shell
shell_build_command = <<EOT
make mpi
EOT

#----------------------------------------------------------------------

[Test build: osu]
test_get = osu
save_stdout_on_success = 1
merge_stdout_stderr = 1
stderr_save_lines = 100

module = Shell
shell_build_command = <<EOT
make
EOT

#----------------------------------------------------------------------

[Test build: imb]
test_get = imb
save_stdout_on_success = 1
merge_stdout_stderr = 1

module = Shell
shell_build_command = <<EOT
cd src
make clean IMB-MPI1
EOT

#----------------------------------------------------------------------

[Test build: skampi]
test_get = skampi
save_stdout_on_success = 1
merge_stdout_stderr = 1
stderr_save_lines = 100

module = Shell
shell_build_command = <<EOT
make
EOT

#======================================================================
# Test Run phase
#======================================================================

[Test run: netpipe]
test_build = netpipe
pass = &and(&cmd_wifexited(), &eq(&cmd_wexitstatus(), 0))
# JMS Need a hueristic for the timeout
timeout = -1
save_stdout_on_pass = 1
# Ensure to leave this value as "-1", or performance results could be lost!
stdout_save_lines = -1
merge_stdout_stderr = 1
np = 2

specify_module = Simple
analyze_module = NetPipe

simple_pass:tests = NPmpi

#----------------------------------------------------------------------

[Test run: osu]
test_build = osu
pass = &and(&cmd_wifexited(), &eq(&cmd_wexitstatus(), 0))
# JMS Need a hueristic for the timeout
timeout = -1
save_stdout_on_pass = 1
# Ensure to leave this value as "-1", or performance results could be lost!
stdout_save_lines = -1
merge_stdout_stderr = 1
np = &env_max_procs()

specify_module = Simple
analyze_module = OSU

simple_pass:tests = osu_bw osu_latency osu_bibw

#----------------------------------------------------------------------

[Test run: imb]
test_build = imb
pass = &and(&cmd_wifexited(), &eq(&cmd_wexitstatus(), 0))
# JMS Need a hueristic for the timeout
timeout = -1
save_stdout_on_pass = 1
# Ensure to leave this value as "-1", or performance results could be lost!
stdout_save_lines = -1
merge_stdout_stderr = 1
np = &env_max_procs()

argv = -npmin &test_np() &enumerate("PingPong", "PingPing", "Sendrecv", "Exchange", "Allreduce", "Reduce", "Reduce_scatter", "Allgather", "Allgatherv", "Alltoall", "Bcast", "Barrier") 

specify_module = Simple
analyze_module = IMB

simple_pass:tests = IMB-MPI1

#----------------------------------------------------------------------

[Test run: skampi]
test_build = skampi
pass = &and(&cmd_wifexited(), &eq(&cmd_wexitstatus(), 0))
# JMS Need a hueristic for the timeout
timeout = -1
save_stdout_on_pass = 1
# Ensure to leave this value as "-1", or performance results could be lost!
stdout_save_lines = -1
merge_stdout_stderr = 1
np = &env_max_procs()

# Need a funclet for it
argv = -i &find("mtt/", "*.ski")

specify_module = Simple
analyze_module = SKaMPI

simple_pass:tests = skampi

#======================================================================
# Reporter phase
#======================================================================

[Reporter: IU database]
module = MTTDatabase

mttdatabase_realm = OMPI
mttdatabase_url = https://www.open-mpi.org/mtt/submit/
# OMPI Core: Change this to be the username and password for your
# submit user.  Get this from the OMPI MTT administrator.
mttdatabase_username = you must set this value
mttdatabase_password = you must set this value
# OMPI Core: Change this to be some short string identifying your
# cluster.
mttdatabase_platform = you must set this value

#----------------------------------------------------------------------

# This is a backup reporter; it also writes results to a local text
# file

[Reporter: text file backup]
module = TextFile

# JMS: this doesn't work
textfile_filename = $phase-$section-$mpi_name-$mpi_version.txt

textfile_summary_header = <<EOT
Hostname: &shell("hostname")
uname: &shell("uname -a")
Username: &shell("who am i")
EOT

textfile_summary_footer =
textfile_detail_header =
textfile_detail_footer =

textfile_textwrap = 78
